{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to /home/hafsa/nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data into x (English sentences) and Y (French sentences)\n",
    "sents = comtrans.aligned_sents('alignment-en-fr.txt')\n",
    "\n",
    "x = []\n",
    "Y = []\n",
    "n = 0\n",
    "\n",
    "for i in sents:\n",
    "    eng = sents[n].words\n",
    "    x.append(eng)\n",
    "    \n",
    "    fr = sents[n].mots\n",
    "    Y.append(fr)\n",
    "    \n",
    "    n += 1\n",
    "    if n == 33334:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23952\n",
      "18762\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary sets\n",
    "\n",
    "set_y = set()\n",
    "for i in Y:\n",
    "    for char in i:\n",
    "        set_y.add(char)\n",
    "\n",
    "print(len(set_y))\n",
    "\n",
    "set_x = set()\n",
    "for i in x:\n",
    "    for char in i:\n",
    "        set_x.add(char)\n",
    "\n",
    "print(len(set_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenized_x = Tokenizer(char_level = False)\n",
    "    tokenized_x.fit_on_texts(x)\n",
    "    return tokenized_x.texts_to_sequences(x), tokenized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "        return pad_sequences(x, maxlen = length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3328    5    1 1019    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(33334, 40)\n",
      "(33334, 40, 1)\n",
      "Data Preprocessed\n",
      "Max English sentence length: 40\n",
      "Max French sentence length: 40\n",
      "English vocabulary size: 17076\n",
      "French vocabulary size: 22820\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(x, y):\n",
    "    \n",
    "    padded_x, tokenized_x = tokenize(x)\n",
    "    padded_y, tokenized_y = tokenize(y)\n",
    "    padded_x = pad(padded_x)\n",
    "    padded_y = pad(padded_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    padded_y = padded_y.reshape(*padded_y.shape, 1)\n",
    "    return padded_x, padded_y, tokenized_x, tokenized_y\n",
    "\n",
    "padded_x, padded_y, tokenized_x, tokenized_y =\\\n",
    "preprocess_text(x, Y)\n",
    "    \n",
    "max_english_sequence_length = padded_x.shape[1]\n",
    "max_french_sequence_length = padded_y.shape[1]\n",
    "english_vocab_size = len(tokenized_x.word_index)+1\n",
    "french_vocab_size = len(tokenized_y.word_index)+1\n",
    "\n",
    "print(padded_x[0])\n",
    "print(padded_x.shape)\n",
    "print(padded_y.shape)\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for simple RNN model\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "#max_french_sequence_length = padded_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(max_french_sequence_length)\n",
    "tmp = pad(padded_x)\n",
    "tmp_x = tmp.reshape((-1, padded_y.shape[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 1054s 39s/step - loss: 5.9565 - accuracy: 0.3844 - val_loss: 3.7988 - val_accuracy: 0.4632\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 1132s 42s/step - loss: 3.8403 - accuracy: 0.4532 - val_loss: 3.7580 - val_accuracy: 0.4684\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 1158s 43s/step - loss: 3.7799 - accuracy: 0.4595 - val_loss: 3.7649 - val_accuracy: 0.4699\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 1173s 44s/step - loss: 3.7776 - accuracy: 0.4578 - val_loss: 3.7820 - val_accuracy: 0.4702\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 1186s 44s/step - loss: 3.7621 - accuracy: 0.4605 - val_loss: 3.7124 - val_accuracy: 0.4718\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 1177s 44s/step - loss: 3.7253 - accuracy: 0.4614 - val_loss: 3.6952 - val_accuracy: 0.4755\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 1153s 43s/step - loss: 3.7033 - accuracy: 0.4633 - val_loss: 3.6809 - val_accuracy: 0.4757\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 1150s 43s/step - loss: 3.6622 - accuracy: 0.4672 - val_loss: 3.7113 - val_accuracy: 0.4762\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 1138s 42s/step - loss: 3.6848 - accuracy: 0.4643 - val_loss: 3.6800 - val_accuracy: 0.4780\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 1113s 41s/step - loss: 3.6507 - accuracy: 0.4678 - val_loss: 3.7359 - val_accuracy: 0.4793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faeb202a070>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network\n",
    "\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, padded_y, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
